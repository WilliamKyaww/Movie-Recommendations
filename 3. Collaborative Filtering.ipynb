{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use collaborative filtering. It is a part of unsupervised learning\n",
    "\n",
    "**Key Steps:**\n",
    "  1. Split the dataset into training (80%) and testing (20%) sets.\n",
    "  2. Train the model using only the training data.\n",
    "  3. Use the model to predict ratings for movies in the test set.\n",
    "  4. Calculate errors between predicted and actual ratings.\n",
    "\n",
    "  If CF is used with only user-item interactions (e.g., movie watch history, clicks, purchases) without explicit labels, itâ€™s considered unsupervised learning.\n",
    "memory-based collaborative filtering\n",
    "  You used ``````, which relies on similarity between movies (item-based filtering) to make recommendations. While this is a Machine Learning technique, it's not a model that \"learns\" from data in the way that deep learning does. Instead, it computes similarities and makes predictions based on existing ratings.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\willi\\OneDrive\\Documents\\GitHub\\Movie-Recommendations\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "script_dir = os.getcwd() \n",
    "\n",
    "print(f\"Current working directory: {script_dir}\")\n",
    "\n",
    "# Load ratings data\n",
    "ratings_file = os.path.join(script_dir, \"Cleaned Datasets\", \"ratings_imdb_matched.csv\")\n",
    "df_ratings = pd.read_csv(ratings_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating User-Item Matrices\n",
    "\n",
    "To perform collaborative filtering, we need to convert our rating data into a **user-item matrix**, where:\n",
    "\n",
    "- **Rows represent users (`userId`)**\n",
    "- **Columns represent movies (`movieId`)**\n",
    "- **Values represent ratings given by users to movies**\n",
    "- Missing values (movies that a user hasn't rated) are filled with `0`.\n",
    "\n",
    "Example User-Item Matrix:\n",
    "\n",
    "| userId | movieId=1 | movieId=2 | movieId=3 | movieId=4 |\n",
    "|--------|----------|----------|----------|----------|\n",
    "| 1      | 4.0      | 0.0      | 3.5      | 5.0      |\n",
    "| 2      | 0.0      | 2.5      | 5.0      | 3.0      |\n",
    "| 3      | 1.0      | 0.0      | 4.0      | 2.0      |\n",
    "\n",
    "The matrix allows us to perform **collaborative filtering** by finding patterns in user ratings. We can then use it to compute **movie similarities** and predict missing ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 80668\n",
      "Test set size: 20168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Shuffle and split dataset into train (80%) and test (20%)\n",
    "train_df, test_df = train_test_split(df_ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# User-item matrices for training and testing\n",
    "train_matrix = train_df.pivot(index=\"userId\", columns=\"imdbId\", values=\"rating\").fillna(0)\n",
    "test_matrix = test_df.pivot(index=\"userId\", columns=\"imdbId\", values=\"rating\").fillna(0)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "train_array = train_matrix.values\n",
    "test_array = test_matrix.values\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Movie Similarities\n",
    "\n",
    "To find how similar two movies are based on user ratings, **cosine similarity** is used. This measures how close two movies are in rating patterns. \n",
    "\n",
    "- **Formula**:  \n",
    "\n",
    " ```math\n",
    "\\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
    "``` \n",
    "where **A** and **B** are rating vectors for two movies.\n",
    "\n",
    "- **Key Steps:**\n",
    "  1. Extract user ratings for each movie.\n",
    "  2. Compute cosine similarity between every pair of movies.\n",
    "  3. Store these values in a **similarity matrix** for quick lookup.\n",
    "\n",
    "This similarity matrix helps in predicting user ratings based on movies they have already rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# Compute cosine similarity manually\n",
    "def cosine_similarity(movie1, movie2):\n",
    "    dot_product = np.dot(movie1, movie2)\n",
    "    norm_product = norm(movie1) * norm(movie2)\n",
    "    return dot_product / norm_product if norm_product != 0 else 0\n",
    "\n",
    "# Create similarity matrix based on the training set\n",
    "num_movies = train_array.shape[1]\n",
    "similarity_matrix_train = np.zeros((num_movies, num_movies))\n",
    "\n",
    "for i in range(num_movies):\n",
    "    for j in range(num_movies):\n",
    "        similarity_matrix_train[i, j] = cosine_similarity(train_array[:, i], train_array[:, j])\n",
    "\n",
    "# Convert to DataFrame\n",
    "movie_similarity_train_df = pd.DataFrame(similarity_matrix_train, index=train_matrix.columns, columns=train_matrix.columns)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell block takes around 30 minutes to run which is far too long, while the code below uses the cosine_similarity function from the sklearn library. This does the exact same thing while only taking just over a second to execute. \n",
    "\n",
    "My original code manually computes cosine similarity using loops, which is inefficient for large datasets.\n",
    "cosine_similarity(train_array.T) from sklearn performs the same computation in a highly optimised way using matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Compute similarity matrix in one step\n",
    "similarity_matrix_train = cosine_similarity(train_array.T)\n",
    "\n",
    "# Convert to DataFrame\n",
    "movie_similarity_train_df = pd.DataFrame(similarity_matrix_train, index=train_matrix.columns, columns=train_matrix.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "Once we have the similarity matrix, we can predict how a user would rate a movie they haven't seen yet. This is done by looking at similar movies they have already rated.\n",
    "\n",
    "- **Key Steps:**\n",
    "  1. Identify movies the user has rated.\n",
    "  2. Find similar movies using the similarity matrix.\n",
    "  3. Compute the weighted average of ratings from similar movies.\n",
    "  4. Predict the rating for the unseen movie.\n",
    "\n",
    "The formula for predicting a rating ```math \\hat{r}_{u,m} ``` \n",
    "for user \\( u \\) and movie \\( m \\) is:\n",
    "\n",
    "```math\n",
    "\\hat{r}_{u,m} = \\frac{\\sum_{n \\in N} \\text{similarity}(m, n) \\times r_{u,n}}{\\sum_{n \\in N} |\\text{similarity}(m, n)|}\n",
    "```\n",
    "\n",
    "\n",
    "where:\n",
    "- \\( N \\) is the set of movies similar to \\( m \\) that the user has rated.\n",
    "- \\( \\text{similarity}(m, n) \\) is the cosine similarity between movies \\( m \\) and \\( n \\).\n",
    "- \\( r_{u,n} \\) is the rating given by user \\( u \\) to movie \\( n \\).\n",
    "\n",
    "This allows us to estimate how much a user might like a movie based on their past ratings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting User Ratings\n",
    "\n",
    "This function, `predict_rating(user_id, movie_id)`, estimates how a user would rate a given movie based on their past ratings and the similarity between movies.\n",
    "\n",
    "#### **How it Works:**\n",
    "1. **Check if the movie exists in the training set:**  \n",
    "   - If the movie is missing, return a default rating of `0`.\n",
    "\n",
    "2. **Retrieve the user's past ratings:**  \n",
    "   - Extract all movies that the user has already rated.\n",
    "\n",
    "3. **Find similar movies:**  \n",
    "   - Get similarity scores between the target movie and other movies the user has rated.\n",
    "\n",
    "4. **Compute a weighted average rating:**  \n",
    "   - Multiply each similarity score by the user's rating for that movie.  \n",
    "   - Sum these weighted values.  \n",
    "   - Normalise by the total similarity sum.\n",
    "\n",
    "5. **Return the predicted rating:**  \n",
    "   - If there are no similar movies, return `0`.  \n",
    "   - Otherwise, return the weighted average rating.\n",
    "\n",
    "#### **Why Use This Approach?**\n",
    "This method is a **memory-based collaborative filtering technique** that makes predictions based on past user behavior. It helps recommend movies that are similar to those the user already likes, improving personalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, movie_id):\n",
    "    # If the movie is not in the training set, return 0\n",
    "    if movie_id not in train_matrix.columns:\n",
    "        return 0  \n",
    "    \n",
    "    # Get movies rated by the user\n",
    "    user_ratings = train_matrix.loc[user_id]\n",
    "    \n",
    "    # Get similarity scores for the target movie\n",
    "    similar_movies = movie_similarity_train_df[movie_id]\n",
    "    \n",
    "    # Compute weighted average of similar movie ratings\n",
    "    weighted_sum = 0\n",
    "    sim_sum = 0\n",
    "    for rated_movie, rating in user_ratings[user_ratings > 0].items():\n",
    "        if rated_movie in similar_movies:\n",
    "            similarity = similar_movies[rated_movie]\n",
    "            weighted_sum += similarity * rating\n",
    "            sim_sum += abs(similarity)\n",
    "    \n",
    "    # Normalise by similarity sum\n",
    "    return weighted_sum / sim_sum if sim_sum != 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "To measure how well our recommendation system performs, we compare its predicted ratings with actual ratings from a test dataset.\n",
    "\n",
    "Common evaluation metrics include calculating the **Mean Absolute Error** (MAE) and the **Root Mean Square Error** (RMSE). Lower MAE and RMSE values indicate better accuracy, meaning our recommendations are closer to actual user preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  **Mean Absolute Error (MAE):** Measures the average absolute difference between predicted and actual ratings.\n",
    "  \n",
    "  ```math\n",
    "    MAE = \\frac{1}{N} \\sum_{i=1}^{N} | \\hat{r}_i - r_i |\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.1627\n"
     ]
    }
   ],
   "source": [
    "# Compute MAE (Mean Absolute Error)\n",
    "actual_ratings = []\n",
    "predicted_ratings = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    user_id = row[\"userId\"]\n",
    "    movie_id = row[\"imdbId\"]\n",
    "    actual_rating = row[\"rating\"]\n",
    "    \n",
    "    predicted_rating = predict_rating(user_id, movie_id)\n",
    "    \n",
    "    actual_ratings.append(actual_rating)\n",
    "    predicted_ratings.append(predicted_rating)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = np.mean(np.abs(np.array(actual_ratings) - np.array(predicted_ratings)))\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  **Root Mean Square Error (RMSE):** Penalises larger errors more heavily.\n",
    "  ```math\n",
    "    RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (\\hat{r}_i - r_i)^2}\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 0.2272\n"
     ]
    }
   ],
   "source": [
    "# Compute RMSE (Root Mean Squared Error) manually\n",
    "squared_errors = [(actual - predicted) ** 2 for actual, predicted in zip(actual_ratings, predicted_ratings)]\n",
    "rmse = np.sqrt(sum(squared_errors) / len(squared_errors))\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "To improve, I can use:\n",
    "- Different similarity metrics (Pearson correlation).\n",
    "- Weighted collaborative filtering (account for user similarity too).\n",
    "- Matrix Factorisation (SVD, ALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using Pearson Correlation Similarity\n",
    "\n",
    "Cosine similarity only considers the angle between vectors, but it does not account for differences in user rating scales. \n",
    "Pearson correlation, on the other hand, measures how well two rating patterns correlate, adjusting for individual biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_11616\\2309430023.py:14: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return pearsonr(movie1[common_users], movie2[common_users])[0]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Compute Pearson correlation similarity\n",
    "def pearson_similarity(movie1, movie2):\n",
    "    common_users = (movie1 > 0) & (movie2 > 0)\n",
    "    if common_users.sum() < 2:\n",
    "        return 0\n",
    "    \n",
    "    # Check for constant arrays\n",
    "    if np.std(movie1[common_users]) == 0 or np.std(movie2[common_users]) == 0:\n",
    "        return 0  # No variation in one or both movies\n",
    "    \n",
    "    try:\n",
    "        return pearsonr(movie1[common_users], movie2[common_users])[0]\n",
    "    except:\n",
    "        return 0  # Handle any other exceptions\n",
    "\n",
    "# Create similarity matrix using Pearson correlation\n",
    "num_movies = train_array.shape[1]\n",
    "pearson_similarity_matrix = np.zeros((num_movies, num_movies))\n",
    "\n",
    "for i in range(num_movies):\n",
    "    for j in range(num_movies):\n",
    "        pearson_similarity_matrix[i, j] = pearson_similarity(train_array[:, i], train_array[:, j])\n",
    "\n",
    "# Convert to DataFrame\n",
    "pearson_similarity_df = pd.DataFrame(pearson_similarity_matrix, index=train_matrix.columns, columns=train_matrix.columns)\n",
    "\n",
    "\n",
    "print(\"Pearson similarity matrix computed!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\willi\\AppData\\Local\\Temp\\ipykernel_8704\\1463596941.py:11: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  return pearsonr(movie1[common_users], movie2[common_users])[0]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Compute Pearson similarity manually\n",
    "def pearson_similarity(movie1, movie2):\n",
    "    # Ignore missing values (zeroes)\n",
    "    common_users = (movie1 > 0) & (movie2 > 0)\n",
    "    \n",
    "    if np.sum(common_users) < 2:  # Need at least 2 common ratings\n",
    "        return 0\n",
    "    \n",
    "    return pearsonr(movie1[common_users], movie2[common_users])[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define num_movies before using it\n",
    "num_movies = train_array.shape[1]  # Number of movies in train_array\n",
    "\n",
    "# Create similarity matrix using Pearson\n",
    "similarity_matrix_pearson = np.zeros((num_movies, num_movies))\n",
    "\n",
    "for i in range(num_movies):\n",
    "    for j in range(num_movies):\n",
    "        similarity_matrix_pearson[i, j] = pearson_similarity(train_array[:, i], train_array[:, j])\n",
    "\n",
    "# Convert to DataFrame\n",
    "movie_similarity_pearson_df = pd.DataFrame(similarity_matrix_pearson, index=train_matrix.columns, columns=train_matrix.columns)\n",
    "\n",
    "print(\"Pearson similarity matrix computed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Compute Pearson similarity manually\n",
    "def pearson_similarity(movie1, movie2):\n",
    "    # Ignore missing values (zeroes)\n",
    "    mask = (movie1 > 0) & (movie2 > 0)\n",
    "    \n",
    "    if np.sum(mask) < 2:  # Need at least 2 common ratings\n",
    "        return 0\n",
    "    \n",
    "    return pearsonr(movie1[mask], movie2[mask])[0]\n",
    "\n",
    "# Define num_movies before using it\n",
    "num_movies = train_array.shape[1]  # Number of movies\n",
    "\n",
    "# Create similarity matrix using Pearson\n",
    "similarity_matrix_pearson = np.zeros((num_movies, num_movies))\n",
    "\n",
    "for i in range(num_movies):\n",
    "    for j in range(num_movies):\n",
    "        similarity_matrix_pearson[i, j] = pearson_similarity(train_array[:, i], train_array[:, j])\n",
    "\n",
    "# Convert to DataFrame\n",
    "movie_similarity_pearson_df = pd.DataFrame(similarity_matrix_pearson, index=train_matrix.columns, columns=train_matrix.columns)\n",
    "\n",
    "print(\"Pearson similarity matrix computed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Weighted Collaborative Filtering\n",
    "\n",
    "Instead of simply averaging ratings of similar movies, we apply a weighted approach where movies with higher similarity scores contribute more to the predicted rating. \n",
    "This helps account for cases where some movies are more closely related than others.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_weighted_rating(user_id, movie_id, similarity_matrix):\n",
    "    if movie_id not in train_matrix.columns:\n",
    "        return 0  \n",
    "\n",
    "    user_ratings = train_matrix.loc[user_id]\n",
    "    similar_movies = similarity_matrix[movie_id]\n",
    "\n",
    "    # Compute weighted sum\n",
    "    weighted_sum = 0\n",
    "    sim_sum = 0\n",
    "    for rated_movie, rating in user_ratings[user_ratings > 0].items():\n",
    "        if rated_movie in similar_movies:\n",
    "            similarity = similar_movies[rated_movie]\n",
    "            weighted_sum += similarity * rating\n",
    "            sim_sum += abs(similarity)\n",
    "\n",
    "    return weighted_sum / sim_sum if sim_sum != 0 else 0\n",
    "\n",
    "# Test with Pearson similarity\n",
    "predicted_rating = predict_weighted_rating(user_id=1, movie_id='tt0133093', similarity_matrix=movie_similarity_pearson_df)\n",
    "print(f\"Predicted rating (Pearson, Weighted): {predicted_rating:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Matrix Factorization (SVD)\n",
    "\n",
    "Memory-based filtering works well but struggles with sparse datasets. \n",
    "SVD (Singular Value Decomposition) reduces the user-movie rating matrix into a lower-dimensional space, revealing hidden relationships. \n",
    "This allows us to make better recommendations even when explicit ratings are missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Decompose train_matrix using SVD\n",
    "U, sigma, Vt = svds(train_matrix, k=50)  # k = number of latent factors\n",
    "\n",
    "# Convert sigma to diagonal matrix\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "# Reconstruct ratings matrix\n",
    "predicted_ratings_matrix = np.dot(np.dot(U, sigma), Vt)\n",
    "\n",
    "# Convert to DataFrame\n",
    "predicted_ratings_df = pd.DataFrame(predicted_ratings_matrix, index=train_matrix.index, columns=train_matrix.columns)\n",
    "\n",
    "print(\"SVD-based rating predictions computed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Predict Ratings using SVD\n",
    "\n",
    "Once we decompose the user-movie matrix into latent factors, we can reconstruct an approximation of the original matrix.\n",
    "This allows us to make predictions based on learned patterns rather than explicit similarity scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_svd_rating(user_id, movie_id):\n",
    "    if movie_id not in predicted_ratings_df.columns:\n",
    "        return 0  \n",
    "\n",
    "    return predicted_ratings_df.loc[user_id, movie_id]\n",
    "\n",
    "predicted_rating_svd = predict_svd_rating(user_id=1, movie_id='tt0133093')\n",
    "print(f\"Predicted rating (SVD): {predicted_rating_svd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Comparing MAE and RMSE Across Methods\n",
    "\n",
    "To evaluate our different approaches, we will calculate MAE and RMSE for:\n",
    "- Cosine similarity (original)\n",
    "- Pearson correlation\n",
    "- Weighted filtering\n",
    "- SVD (Matrix Factorization)\n",
    "\n",
    "Lower MAE/RMSE values indicate better prediction accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predict_function, similarity_matrix=None):\n",
    "    actual_ratings = []\n",
    "    predicted_ratings = []\n",
    "\n",
    "    for _, row in test_df.iterrows():\n",
    "        user_id = row[\"userId\"]\n",
    "        movie_id = row[\"imdbId\"]\n",
    "        actual_rating = row[\"rating\"]\n",
    "\n",
    "        if similarity_matrix is not None:\n",
    "            predicted_rating = predict_function(user_id, movie_id, similarity_matrix)\n",
    "        else:\n",
    "            predicted_rating = predict_function(user_id, movie_id)\n",
    "\n",
    "        actual_ratings.append(actual_rating)\n",
    "        predicted_ratings.append(predicted_rating)\n",
    "\n",
    "    actual_ratings = np.array(actual_ratings)\n",
    "    predicted_ratings = np.array(predicted_ratings)\n",
    "\n",
    "    mae = np.mean(np.abs(actual_ratings - predicted_ratings))\n",
    "    rmse = np.sqrt(np.mean((actual_ratings - predicted_ratings) ** 2))\n",
    "\n",
    "    return mae, rmse\n",
    "\n",
    "# Evaluate all models\n",
    "models = {\n",
    "    \"Cosine Similarity\": (predict_rating, movie_similarity_train_df),\n",
    "    \"Pearson Correlation\": (predict_weighted_rating, movie_similarity_pearson_df),\n",
    "    \"SVD Matrix Factorization\": (predict_svd_rating, None),\n",
    "}\n",
    "\n",
    "for model_name, (predict_func, sim_matrix) in models.items():\n",
    "    mae, rmse = evaluate_model(predict_func, sim_matrix)\n",
    "    print(f\"{model_name} - MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
