{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use collaborative filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\willi\\OneDrive\\Documents\\GitHub\\Test\\Movie-Recommendation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "script_dir = os.getcwd() \n",
    "\n",
    "print(f\"Current working directory: {script_dir}\")\n",
    "\n",
    "# Load ratings data\n",
    "ratings_file = os.path.join(script_dir, \"Cleaned Datasets\", \"ratings_imdb_matched.csv\")\n",
    "df_ratings = pd.read_csv(ratings_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating User-Item Matrices\n",
    "\n",
    "To perform collaborative filtering, we need to convert our rating data into a **user-item matrix**, where:\n",
    "\n",
    "- **Rows represent users (`userId`)**\n",
    "- **Columns represent movies (`movieId`)**\n",
    "- **Values represent ratings given by users to movies**\n",
    "- Missing values (movies that a user hasn't rated) are filled with `0`.\n",
    "\n",
    "Example User-Item Matrix:\n",
    "\n",
    "| userId | movieId=1 | movieId=2 | movieId=3 | movieId=4 |\n",
    "|--------|----------|----------|----------|----------|\n",
    "| 1      | 4.0      | 0.0      | 3.5      | 5.0      |\n",
    "| 2      | 0.0      | 2.5      | 5.0      | 3.0      |\n",
    "| 3      | 1.0      | 0.0      | 4.0      | 2.0      |\n",
    "\n",
    "The matrix allows us to perform **collaborative filtering** by finding patterns in user ratings. We can then use it to compute **movie similarities** and predict missing ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 80668\n",
      "Test set size: 20168\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Shuffle and split dataset into train (80%) and test (20%)\n",
    "train_df, test_df = train_test_split(df_ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# User-item matrices for training and testing\n",
    "train_matrix = train_df.pivot(index=\"userId\", columns=\"imdbId\", values=\"rating\").fillna(0)\n",
    "test_matrix = test_df.pivot(index=\"userId\", columns=\"imdbId\", values=\"rating\").fillna(0)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "train_array = train_matrix.values\n",
    "test_array = test_matrix.values\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Movie Similarities\n",
    "\n",
    "To find how similar two movies are based on user ratings, **cosine similarity** is used. This measures how close two movies are in rating patterns. \n",
    "\n",
    "- **Formula**:  \n",
    "\n",
    " ```math\n",
    "\\cos(\\theta) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}\n",
    "``` \n",
    "where **A** and **B** are rating vectors for two movies.\n",
    "\n",
    "- **Key Steps:**\n",
    "  1. Extract user ratings for each movie.\n",
    "  2. Compute cosine similarity between every pair of movies.\n",
    "  3. Store these values in a **similarity matrix** for quick lookup.\n",
    "\n",
    "This similarity matrix helps in predicting user ratings based on movies they have already rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "# Compute cosine similarity manually\n",
    "def cosine_similarity(movie1, movie2):\n",
    "    dot_product = np.dot(movie1, movie2)\n",
    "    norm_product = norm(movie1) * norm(movie2)\n",
    "    return dot_product / norm_product if norm_product != 0 else 0\n",
    "\n",
    "# Create similarity matrix based on the training set\n",
    "num_movies = train_array.shape[1]\n",
    "similarity_matrix_train = np.zeros((num_movies, num_movies))\n",
    "\n",
    "for i in range(num_movies):\n",
    "    for j in range(num_movies):\n",
    "        similarity_matrix_train[i, j] = cosine_similarity(train_array[:, i], train_array[:, j])\n",
    "\n",
    "# Convert to DataFrame\n",
    "movie_similarity_train_df = pd.DataFrame(similarity_matrix_train, index=train_matrix.columns, columns=train_matrix.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "Once we have the similarity matrix, we can predict how a user would rate a movie they haven't seen yet. This is done by looking at similar movies they have already rated.\n",
    "\n",
    "- **Key Steps:**\n",
    "  1. Identify movies the user has rated.\n",
    "  2. Find similar movies using the similarity matrix.\n",
    "  3. Compute the weighted average of ratings from similar movies.\n",
    "  4. Predict the rating for the unseen movie.\n",
    "\n",
    "The formula for predicting a rating \\( \\hat{r}_{u,m} \\) for user \\( u \\) and movie \\( m \\) is:\n",
    "\n",
    "```math\n",
    "\\hat{r}_{u,m} = \\frac{\\sum_{n \\in N} \\text{similarity}(m, n) \\times r_{u,n}}{\\sum_{n \\in N} |\\text{similarity}(m, n)|}\n",
    "```\n",
    "\n",
    "where:\n",
    "- \\( N \\) is the set of movies similar to \\( m \\) that the user has rated.\n",
    "- \\( \\text{similarity}(m, n) \\) is the cosine similarity between movies \\( m \\) and \\( n \\).\n",
    "- \\( r_{u,n} \\) is the rating given by user \\( u \\) to movie \\( n \\).\n",
    "\n",
    "This allows us to estimate how much a user might like a movie based on their past ratings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rating(user_id, movie_id):\n",
    "    # If the movie is not in the training set, return 0\n",
    "    if movie_id not in train_matrix.columns:\n",
    "        return 0  \n",
    "    \n",
    "    # Get movies rated by the user\n",
    "    user_ratings = train_matrix.loc[user_id]\n",
    "    \n",
    "    # Get similarity scores for the target movie\n",
    "    similar_movies = movie_similarity_train_df[movie_id]\n",
    "    \n",
    "    # Compute weighted average of similar movie ratings\n",
    "    weighted_sum = 0\n",
    "    sim_sum = 0\n",
    "    for rated_movie, rating in user_ratings[user_ratings > 0].items():\n",
    "        if rated_movie in similar_movies:\n",
    "            similarity = similar_movies[rated_movie]\n",
    "            weighted_sum += similarity * rating\n",
    "            sim_sum += abs(similarity)\n",
    "    \n",
    "    # Normalize by similarity sum\n",
    "    return weighted_sum / sim_sum if sim_sum != 0 else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "To measure how well our recommendation system performs, we compare its predicted ratings with actual ratings from a test dataset.\n",
    "\n",
    "- **Key Steps:**\n",
    "  1. **Split the dataset** into training (80%) and testing (20%) sets.\n",
    "  2. Train the model using only the training data.\n",
    "  3. Use the model to predict ratings for movies in the test set.\n",
    "  4. Calculate errors between predicted and actual ratings.\n",
    "\n",
    "- **Common Evaluation Metrics:**\n",
    "  - **Mean Absolute Error (MAE):** Measures the average absolute difference between predicted and actual ratings.\n",
    "    \\[\n",
    "    MAE = \\frac{1}{N} \\sum_{i=1}^{N} | \\hat{r}_i - r_i |\n",
    "    \\]\n",
    "  - **Root Mean Square Error (RMSE):** Penalizes larger errors more heavily.\n",
    "    \\[\n",
    "    RMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (\\hat{r}_i - r_i)^2}\n",
    "    \\]\n",
    "\n",
    "Lower MAE and RMSE values indicate better accuracy, meaning our recommendations are closer to actual user preferences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.1627\n"
     ]
    }
   ],
   "source": [
    "# Compute MAE (Mean Absolute Error)\n",
    "actual_ratings = []\n",
    "predicted_ratings = []\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    user_id = row[\"userId\"]\n",
    "    movie_id = row[\"imdbId\"]\n",
    "    actual_rating = row[\"rating\"]\n",
    "    \n",
    "    predicted_rating = predict_rating(user_id, movie_id)\n",
    "    \n",
    "    actual_ratings.append(actual_rating)\n",
    "    predicted_ratings.append(predicted_rating)\n",
    "\n",
    "# Calculate MAE\n",
    "mae = np.mean(np.abs(np.array(actual_ratings) - np.array(predicted_ratings)))\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
